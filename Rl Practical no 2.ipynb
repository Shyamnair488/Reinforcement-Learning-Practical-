{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMdYD/o7+Vv6D9vFwmKlpoc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Markov Decision Process"],"metadata":{"id":"zU0Z63gWYldV"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Define the MDP parameters\n","num_states = 3\n","num_actions = 2\n","discount_factor = 0.9\n","\n","# Define the MDP transition probabilities\n","# transition_probs[state][action][next_state]\n","transition_probs = np.array([\n","    [[0.7, 0.3, 0.0], [0.0, 1.0, 0.0]],  # From state 0\n","    [[0.0, 0.8, 0.2], [0.0, 0.0, 1.0]],  # From state 1\n","    [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]]   # From state 2\n","])\n","\n","# Define the rewards\n","# rewards[state][action]\n","rewards = np.array([\n","    [10, 0],  # From state 0\n","    [0, 1],   # From state 1\n","    [0, 0]    # From state 2\n","])\n","\n","# Print the transition probabilities\n","print(\"Transition Probabilities:\")\n","for s in range(num_states):\n","    for a in range(num_actions):\n","        print(f\"State {s}, Action {a}:\")\n","        for next_state in range(num_states):\n","            transition_prob = transition_probs[s][a][next_state]\n","            print(f\"  -> State {next_state} with Probability {transition_prob}\")\n","        print()\n","\n","# Print the rewards\n","print(\"Rewards:\")\n","for s in range(num_states):\n","    for a in range(num_actions):\n","        reward = rewards[s][a]\n","        print(f\"State {s}, Action {a} -> Reward {reward}\")\n","\n","# Initialize the value function arbitrarily\n","value_function = np.zeros(num_states)\n","\n","# Perform Value Iteration\n","num_iterations = 100\n","for _ in range(num_iterations):\n","    new_value_function = np.zeros(num_states)\n","    for s in range(num_states):\n","        for a in range(num_actions):\n","            expected_return = sum(\n","                transition_probs[s][a][s_prime] * (rewards[s][a] + discount_factor * value_function[s_prime])\n","                for s_prime in range(num_states)\n","            )\n","            new_value_function[s] = max(new_value_function[s], expected_return)\n","    value_function = new_value_function\n","\n","# Calculate the optimal policy\n","policy = np.zeros(num_states, dtype=int)\n","for s in range(num_states):\n","    action_values = [\n","        sum(\n","            transition_probs[s][a][s_prime] * (rewards[s][a] + discount_factor * value_function[s_prime])\n","            for s_prime in range(num_states)\n","        )\n","        for a in range(num_actions)\n","    ]\n","    policy[s] = np.argmax(action_values)\n","\n","# Display the results for all states\n","for s in range(num_states):\n","    print(f\"State {s}:\")\n","    print(\"Value Function:\", value_function[s])\n","    print(\"Optimal Policy (Action):\", policy[s])\n","    print()\n","\n","# Print the results\n","print(\"Optimal Value Function:\", value_function)\n","print(\"Optimal Policy:\", policy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"lHhMi7xwbcR5","executionInfo":{"status":"ok","timestamp":1694069937148,"user_tz":-330,"elapsed":436,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}},"outputId":"7a689138-ff0b-4afd-ca1f-085d5a417681"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Transition Probabilities:\n","State 0, Action 0:\n","  -> State 0 with Probability 0.7\n","  -> State 1 with Probability 0.3\n","  -> State 2 with Probability 0.0\n","\n","State 0, Action 1:\n","  -> State 0 with Probability 0.0\n","  -> State 1 with Probability 1.0\n","  -> State 2 with Probability 0.0\n","\n","State 1, Action 0:\n","  -> State 0 with Probability 0.0\n","  -> State 1 with Probability 0.8\n","  -> State 2 with Probability 0.2\n","\n","State 1, Action 1:\n","  -> State 0 with Probability 0.0\n","  -> State 1 with Probability 0.0\n","  -> State 2 with Probability 1.0\n","\n","State 2, Action 0:\n","  -> State 0 with Probability 0.0\n","  -> State 1 with Probability 0.0\n","  -> State 2 with Probability 1.0\n","\n","State 2, Action 1:\n","  -> State 0 with Probability 0.0\n","  -> State 1 with Probability 0.0\n","  -> State 2 with Probability 1.0\n","\n","Rewards:\n","State 0, Action 0 -> Reward 10\n","State 0, Action 1 -> Reward 0\n","State 1, Action 0 -> Reward 0\n","State 1, Action 1 -> Reward 1\n","State 2, Action 0 -> Reward 0\n","State 2, Action 1 -> Reward 0\n","State 0:\n","Value Function: 27.756756756756747\n","Optimal Policy (Action): 0\n","\n","State 1:\n","Value Function: 1.0\n","Optimal Policy (Action): 1\n","\n","State 2:\n","Value Function: 0.0\n","Optimal Policy (Action): 0\n","\n","Optimal Value Function: [27.75675676  1.          0.        ]\n","Optimal Policy: [0 1 0]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Rh-suRZGbq7E"},"execution_count":null,"outputs":[]}]}