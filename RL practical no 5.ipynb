{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOviTwj8TqQ9xpfGUIqXDhY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Implementing a parallel Q-learning algorithm"],"metadata":{"id":"NK0AGcXelsbN"}},{"cell_type":"code","source":["pip install gym\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"XaaNWak4nNIb","executionInfo":{"status":"ok","timestamp":1695281031638,"user_tz":-330,"elapsed":5009,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}},"outputId":"34a74611-8bf8-4fcc-8808-909021fb6bf5"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"]}]},{"cell_type":"code","source":["import gym\n","import numpy as np"],"metadata":{"id":"gGdVAKPZno2H","executionInfo":{"status":"ok","timestamp":1695281114369,"user_tz":-330,"elapsed":7,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# Create the FrozenLake environment\n","env = gym.make(\"FrozenLake-v1\")"],"metadata":{"id":"fJvUx1eInqeP","executionInfo":{"status":"ok","timestamp":1695281114369,"user_tz":-330,"elapsed":7,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# Initialize the Q-table with zeros\n","num_states = env.observation_space.n\n","num_actions = env.action_space.n\n","Q = np.zeros((num_states, num_actions))"],"metadata":{"id":"7np5wm4enrut","executionInfo":{"status":"ok","timestamp":1695281114369,"user_tz":-330,"elapsed":6,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","learning_rate = 0.1\n","discount_factor = 0.0099\n","num_episodes = 10000\n","max_steps_per_episode = 100"],"metadata":{"id":"_m0yCsZQnu3O","executionInfo":{"status":"ok","timestamp":1695281282186,"user_tz":-330,"elapsed":520,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["# Exploration parameters\n","epsilon = 1.0\n","epsilon_decay = 0.995\n","min_epsilon = 0.01"],"metadata":{"id":"vvynq7SVnwSV","executionInfo":{"status":"ok","timestamp":1695281197917,"user_tz":-330,"elapsed":481,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["# Q-learning algorithm\n","for episode in range(num_episodes):\n","    state = env.reset()\n","    done = False\n","    total_reward = 0\n","\n","    for step in range(max_steps_per_episode):\n","        # Epsilon-greedy policy for action selection\n","        if np.random.rand() < epsilon:\n","            action = env.action_space.sample()  # Explore\n","        else:\n","            action = np.argmax(Q[state, :])  # Exploit\n","\n","        # Take the selected action and observe the new state and reward\n","        next_state, reward, done, _ = env.step(action)\n","\n","        # Update the Q-table\n","        Q[state, action] = (1 - learning_rate) * Q[state, action] + \\\n","            learning_rate * (reward + discount_factor * np.max(Q[next_state, :]))\n","\n","        total_reward += reward\n","        state = next_state\n","\n","        if done:\n","            break\n","\n","    # Decay epsilon to reduce exploration over time\n","    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n","\n","    if episode % 100 == 0:\n","        print(f\"Episode {episode}, Total Reward: {total_reward}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fICei2hBn0Px","executionInfo":{"status":"ok","timestamp":1695281288081,"user_tz":-330,"elapsed":3287,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}},"outputId":"df1c5031-009e-46ad-e936-43c23983511b"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 0, Total Reward: 0.0\n","Episode 100, Total Reward: 0.0\n","Episode 200, Total Reward: 0.0\n","Episode 300, Total Reward: 0.0\n","Episode 400, Total Reward: 0.0\n","Episode 500, Total Reward: 0.0\n","Episode 600, Total Reward: 0.0\n","Episode 700, Total Reward: 0.0\n","Episode 800, Total Reward: 0.0\n","Episode 900, Total Reward: 0.0\n","Episode 1000, Total Reward: 0.0\n","Episode 1100, Total Reward: 0.0\n","Episode 1200, Total Reward: 0.0\n","Episode 1300, Total Reward: 0.0\n","Episode 1400, Total Reward: 0.0\n","Episode 1500, Total Reward: 0.0\n","Episode 1600, Total Reward: 0.0\n","Episode 1700, Total Reward: 0.0\n","Episode 1800, Total Reward: 1.0\n","Episode 1900, Total Reward: 0.0\n","Episode 2000, Total Reward: 0.0\n","Episode 2100, Total Reward: 0.0\n","Episode 2200, Total Reward: 0.0\n","Episode 2300, Total Reward: 0.0\n","Episode 2400, Total Reward: 0.0\n","Episode 2500, Total Reward: 0.0\n","Episode 2600, Total Reward: 0.0\n","Episode 2700, Total Reward: 0.0\n","Episode 2800, Total Reward: 0.0\n","Episode 2900, Total Reward: 0.0\n","Episode 3000, Total Reward: 0.0\n","Episode 3100, Total Reward: 0.0\n","Episode 3200, Total Reward: 0.0\n","Episode 3300, Total Reward: 0.0\n","Episode 3400, Total Reward: 1.0\n","Episode 3500, Total Reward: 0.0\n","Episode 3600, Total Reward: 0.0\n","Episode 3700, Total Reward: 0.0\n","Episode 3800, Total Reward: 0.0\n","Episode 3900, Total Reward: 1.0\n","Episode 4000, Total Reward: 0.0\n","Episode 4100, Total Reward: 0.0\n","Episode 4200, Total Reward: 0.0\n","Episode 4300, Total Reward: 0.0\n","Episode 4400, Total Reward: 0.0\n","Episode 4500, Total Reward: 0.0\n","Episode 4600, Total Reward: 0.0\n","Episode 4700, Total Reward: 0.0\n","Episode 4800, Total Reward: 0.0\n","Episode 4900, Total Reward: 0.0\n","Episode 5000, Total Reward: 0.0\n","Episode 5100, Total Reward: 0.0\n","Episode 5200, Total Reward: 0.0\n","Episode 5300, Total Reward: 0.0\n","Episode 5400, Total Reward: 0.0\n","Episode 5500, Total Reward: 0.0\n","Episode 5600, Total Reward: 0.0\n","Episode 5700, Total Reward: 0.0\n","Episode 5800, Total Reward: 1.0\n","Episode 5900, Total Reward: 0.0\n","Episode 6000, Total Reward: 1.0\n","Episode 6100, Total Reward: 0.0\n","Episode 6200, Total Reward: 0.0\n","Episode 6300, Total Reward: 0.0\n","Episode 6400, Total Reward: 0.0\n","Episode 6500, Total Reward: 0.0\n","Episode 6600, Total Reward: 0.0\n","Episode 6700, Total Reward: 0.0\n","Episode 6800, Total Reward: 0.0\n","Episode 6900, Total Reward: 0.0\n","Episode 7000, Total Reward: 0.0\n","Episode 7100, Total Reward: 0.0\n","Episode 7200, Total Reward: 0.0\n","Episode 7300, Total Reward: 0.0\n","Episode 7400, Total Reward: 0.0\n","Episode 7500, Total Reward: 0.0\n","Episode 7600, Total Reward: 0.0\n","Episode 7700, Total Reward: 0.0\n","Episode 7800, Total Reward: 0.0\n","Episode 7900, Total Reward: 0.0\n","Episode 8000, Total Reward: 0.0\n","Episode 8100, Total Reward: 0.0\n","Episode 8200, Total Reward: 0.0\n","Episode 8300, Total Reward: 0.0\n","Episode 8400, Total Reward: 0.0\n","Episode 8500, Total Reward: 0.0\n","Episode 8600, Total Reward: 0.0\n","Episode 8700, Total Reward: 0.0\n","Episode 8800, Total Reward: 0.0\n","Episode 8900, Total Reward: 0.0\n","Episode 9000, Total Reward: 0.0\n","Episode 9100, Total Reward: 0.0\n","Episode 9200, Total Reward: 0.0\n","Episode 9300, Total Reward: 0.0\n","Episode 9400, Total Reward: 0.0\n","Episode 9500, Total Reward: 0.0\n","Episode 9600, Total Reward: 0.0\n","Episode 9700, Total Reward: 0.0\n","Episode 9800, Total Reward: 0.0\n","Episode 9900, Total Reward: 0.0\n"]}]},{"cell_type":"code","source":["# Evaluate the trained Q-table\n","num_evaluation_episodes = 100\n","total_rewards = []\n","\n","for episode in range(num_evaluation_episodes):\n","    state = env.reset()\n","    done = False\n","    episode_reward = 0\n","\n","    while not done:\n","        action = np.argmax(Q[state, :])\n","        state, reward, done, _ = env.step(action)\n","        episode_reward += reward\n","\n","    total_rewards.append(episode_reward)\n","\n","average_reward = np.mean(total_rewards)\n","print(f\"Average Reward Over {num_evaluation_episodes} Episodes: {average_reward}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hVWX-w1ynXMF","executionInfo":{"status":"ok","timestamp":1695281217027,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}},"outputId":"287fd4cb-ab3a-4577-d60e-d6d61b1c9667"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Reward Over 100 Episodes: 0.75\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"yqLNBwsQoi_G"},"execution_count":null,"outputs":[]}]}