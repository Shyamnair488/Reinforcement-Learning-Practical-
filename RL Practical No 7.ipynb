{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMPuZ4pti34BZb2Dww9fJP6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Aim : Implement TIC-TA-TOE using RL"],"metadata":{"id":"D1SfSApi3oay"}},{"cell_type":"code","source":["import numpy as np\n","import random"],"metadata":{"id":"7lr_3JKZ3LUr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Constants\n","EMPTY = 0\n","PLAYER_X = 1\n","PLAYER_O = -1\n","BOARD_SIZE = 3\n","NUM_EPISODES = 10000\n","LEARNING_RATE = 0.1\n","DISCOUNT_FACTOR = 0.9\n","EPSILON = 0.1"],"metadata":{"id":"Of8VueL03M2r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the Tic-Tac-Toe environment\n","class TicTacToe:\n","    def __init__(self):\n","        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE))\n","        self.current_player = PLAYER_X\n","        self.winner = None\n","        self.done = False\n","\n","    def reset(self):\n","        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE))\n","        self.current_player = PLAYER_X\n","        self.winner = None\n","        self.done = False\n","\n","    def is_valid_move(self, move):\n","        row, col = move\n","        return self.board[row][col] == EMPTY\n","\n","    def make_move(self, move):\n","        if self.done:\n","            return\n","        row, col = move\n","        if self.is_valid_move(move):\n","            self.board[row][col] = self.current_player\n","            self.check_winner()\n","            self.current_player = -self.current_player\n","\n","    def check_winner(self):\n","        for i in range(BOARD_SIZE):\n","            row_sum = sum(self.board[i, :])\n","            col_sum = sum(self.board[:, i])\n","            if row_sum == BOARD_SIZE or col_sum == BOARD_SIZE:\n","                self.winner = PLAYER_X\n","                self.done = True\n","                return\n","            if row_sum == -BOARD_SIZE or col_sum == -BOARD_SIZE:\n","                self.winner = PLAYER_O\n","                self.done = True\n","                return\n","\n","        diag_sum1 = sum(self.board[i][i] for i in range(BOARD_SIZE))\n","        diag_sum2 = sum(self.board[i][BOARD_SIZE - 1 - i] for i in range(BOARD_SIZE))\n","        if diag_sum1 == BOARD_SIZE or diag_sum2 == BOARD_SIZE:\n","            self.winner = PLAYER_X\n","            self.done = True\n","        if diag_sum1 == -BOARD_SIZE or diag_sum2 == -BOARD_SIZE:\n","            self.winner = PLAYER_O\n","            self.done = True\n","\n","    def get_state(self):\n","        return tuple(tuple(row) for row in self.board)\n","\n","    def print_board(self):\n","        for row in self.board:\n","            print(\" \".join([\"X\" if cell == PLAYER_X else \"O\" if cell == PLAYER_O else \"-\" for cell in row]))\n","\n","# Q-learning agent\n","class QLearningAgent:\n","    def __init__(self, epsilon=EPSILON, learning_rate=LEARNING_RATE, discount_factor=DISCOUNT_FACTOR):\n","        self.q_table = {}\n","        self.epsilon = epsilon\n","        self.learning_rate = learning_rate\n","        self.discount_factor = discount_factor\n","\n","    def get_q_value(self, state, action):\n","        if (state, action) not in self.q_table:\n","            return 0\n","        return self.q_table[(state, action)]\n","\n","    def choose_action(self, state):\n","        available_moves = [(i, j) for i in range(BOARD_SIZE) for j in range(BOARD_SIZE) if state[i][j] == EMPTY]\n","\n","        if len(available_moves) == 0:\n","            return None  # No valid moves\n","\n","        if random.random() < self.epsilon:\n","            # Explore: choose a random valid move\n","            return random.choice(available_moves)\n","        else:\n","            # Exploit: choose the action with the highest Q-value\n","            best_action = None\n","            best_q_value = -float('inf')\n","            for i, j in available_moves:\n","                action = (i, j)\n","                q_value = self.get_q_value(state, action)\n","                if q_value > best_q_value:\n","                    best_action = action\n","                    best_q_value = q_value\n","            return best_action\n","\n","    def update_q_value(self, state, action, reward, next_state):\n","        if state not in self.q_table:\n","            self.q_table[state] = {}\n","        if next_state not in self.q_table:\n","            self.q_table[next_state] = {}\n","\n","        best_next_action = self.choose_action(next_state)\n","        q_value = self.get_q_value(state, action)\n","        next_q_value = self.get_q_value(next_state, best_next_action)\n","\n","        updated_q_value = (1 - self.learning_rate) * q_value + self.learning_rate * (reward + self.discount_factor * next_q_value)\n","        self.q_table[state][action] = updated_q_value\n"],"metadata":{"id":"vPxsUdIQ3Rfd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training the agent using Q-learning\n","def train_q_learning_agent():\n","    agent = QLearningAgent()\n","    env = TicTacToe()\n","    for episode in range(NUM_EPISODES):\n","        env.reset()\n","        state = env.get_state()\n","\n","        while not env.done:\n","            action = agent.choose_action(state)\n","            if action is None:\n","                break  # No valid moves\n","            env.make_move(action)\n","            next_state = env.get_state()\n","            if env.done:\n","                if env.winner == PLAYER_X:\n","                    agent.update_q_value(state, action, 1, next_state)\n","                elif env.winner == PLAYER_O:\n","                    agent.update_q_value(state, action, -1, next_state)\n","                else:\n","                    agent.update_q_value(state, action, 0, next_state)\n","            else:\n","                agent.update_q_value(state, action, 0, next_state)\n","            state = next_state\n","\n","    return agent"],"metadata":{"id":"l2bBfsSw3UFh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Play a game with the trained agent\n","def play_game(agent):\n","    env = TicTacToe()\n","    env.reset()\n","    state = env.get_state()\n","\n","    while not env.done:\n","        env.print_board()\n","        print(\"Current player: \" + (\"X\" if env.current_player == PLAYER_X else \"O\"))\n","        if env.current_player == PLAYER_X:\n","            action = agent.choose_action(state)\n","        else:\n","            while True:\n","                row = int(input(\"Enter row (0, 1, 2): \"))\n","                col = int(input(\"Enter column (0, 1, 2): \"))\n","                action = (row, col)\n","                if env.is_valid_move(action):\n","                    break\n","                else:\n","                    print(\"Invalid move. Try again.\")\n","        env.make_move(action)\n","        state = env.get_state()\n","\n","    env.print_board()\n","    if env.winner == PLAYER_X:\n","        print(\"X wins!\")\n","    elif env.winner == PLAYER_O:\n","        print(\"O wins!\")\n","    else:\n","        print(\"It's a tie!\")"],"metadata":{"id":"uHBZqbKI3W7L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    trained_agent = train_q_learning_agent()\n","    play_game(trained_agent)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cNDmrV9X2tlX","executionInfo":{"status":"ok","timestamp":1697080279299,"user_tz":-330,"elapsed":28154,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}},"outputId":"740ad90c-dd1d-4d26-f6ac-571436771d09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["- - -\n","- - -\n","- - -\n","Current player: X\n","X - -\n","- - -\n","- - -\n","Current player: O\n","Enter row (0, 1, 2): 1\n","Enter column (0, 1, 2): 1\n","X - -\n","- O -\n","- - -\n","Current player: X\n","X X -\n","- O -\n","- - -\n","Current player: O\n","Enter row (0, 1, 2): 0\n","Enter column (0, 1, 2): 2\n","X X O\n","- O -\n","- - -\n","Current player: X\n","X X O\n","X O -\n","- - -\n","Current player: O\n","Enter row (0, 1, 2): 2\n","Enter column (0, 1, 2): 0\n","X X O\n","X O -\n","O - -\n","O wins!\n"]}]}]}